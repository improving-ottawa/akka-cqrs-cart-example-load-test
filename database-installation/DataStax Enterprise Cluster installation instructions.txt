
-------------------------------------------------------------------------------------
-- STEP-0: General information & Preparations
-------------------------------------------------------------------------------------

Authenticate your gcloud SDK from your local machine:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
REF: https://cloud.google.com/artifact-registry/docs/docker/authentication

gcloud auth login


Set the right project:
~~~~~~~~~~~~~~~~~~~~~~~
gcloud projects list |grep scylla
gcloud config set project scylla-cassandra-comparison


DSE Configuration:
~~~~~~~~~~~~~~~~~~~~
General configuration: https://docs.datastax.com/en/dse/6.8/dse-dev/datastax_enterprise/config/configTOC.html

For recommended production settings: https://docs.datastax.com/en/dse/6.8/dse-dev/datastax_enterprise/config/configRecommendedSettings.html

Recommended capacity planning guide for DSE: https://docs.datastax.com/en/dseplanning/docs/capacityPlanning.html


Clearing the data from DataStax Enterprise:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
https://docs.datastax.com/en/dse/6.8/dse-dev/datastax_enterprise/operations/opsClearData.html

Please refer to Package installation steps. Please also see the "Stop" details before clearing data.


Configuration files in the DSE node:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Configuration files are available at '/etc/dse/cassandra/' in a DSE node.


Log files in the DSE node:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Log files are available at '/var/log/cassandra/' in a DSE node.


Starting and stopping DSE cluster node from command-line:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
sudo service dse start
sudo service dse stop


DataStax enterprise license terms:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
https://www.datastax.com/legal/datastax-enterprise-terms


See list of available Ubuntu images in GCP:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gcloud compute images list --filter ubuntu-os-cloud

... ... ...
... ... ...
NAME: ubuntu-1804-bionic-v20230308
PROJECT: ubuntu-os-cloud
FAMILY: ubuntu-1804-lts
DEPRECATED:
STATUS: READY
... ... ...
... ... ...
NAME: ubuntu-2004-focal-v20230302
PROJECT: ubuntu-os-cloud
FAMILY: ubuntu-2004-lts
DEPRECATED:
STATUS: READY
... ... ...
... ... ...


-------------------------------------------------------------------------------------
-- STEP-1: Deploy DSE OpsCenter in GCP
-------------------------------------------------------------------------------------


Launch the Ubuntu 18.04 LST based VM in GCP:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gcloud compute instances create opscenter --image ubuntu-1804-bionic-v20230308 --image-project ubuntu-os-cloud --local-ssd interface=nvme --machine-type=n2-highmem-2 --zone=northamerica-northeast2-a


From SSH terminal issue the following commands:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
$> sudo apt-get update
$> sudo apt update


Install OpenJDK 8 install on Ubuntu:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
REF: https://docs.datastax.com/en/jdk-install/doc/jdk-install/installOpenJdkDeb.html

$> sudo apt-get install openjdk-8-jdk



Install DSE OpsCenter on GCP VM:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
REF: https://docs.datastax.com/en/installing/docs/opscInstallDeb.html

Once installed, configure firewall to enable http & https traffic and ingress on port 8888

And then launch the OpsCenter UI @ http://opscenter-host:8888/  where 'opscenter-host' is the public IP address of VM instance where you installed the DSE OpsCenter.




-- -------------------------------------------------------------------------------
-- STEP-2: Instanciate & prepare 3 VMs in GCP for the DataStax Enterprise cluster
-- --------------------------------------------------------------------------------

Issue the following commands via CLOUD SHELL Terminal in GCP Console.

gcloud compute instances create dse-1 --image ubuntu-2004-focal-v20230302 --image-project ubuntu-os-cloud --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --machine-type=n2-highmem-4 --zone=northamerica-northeast2-a

gcloud compute instances create dse-2 --image ubuntu-2004-focal-v20230302 --image-project ubuntu-os-cloud --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --machine-type=n2-highmem-4 --zone=northamerica-northeast2-a

gcloud compute instances create dse-3 --image ubuntu-2004-focal-v20230302 --image-project ubuntu-os-cloud --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --machine-type=n2-highmem-4 --zone=northamerica-northeast2-a


Configure the VMs for SSH connection:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# If not already done, Generate SSH keys on your local machine:
ssh-keygen -t rsa -f ~/.ssh/gcloud-ssh-dse-vms -C syed.junaid@yoppworks.com
ssh-keygen -p -N "" -m pem -f ~/.ssh/gcloud-ssh-dse-vms

# Transfer Public Key to the VM instance(s) once they are created:
# Print public key
$> cat ~/.ssh/gcloud-ssh-dse-vms.pub

# <Edit> your VM instance in Google Cloud console and select <ADD ITEM> under 'SSH Keys', and copy your Public Key (from previous step) there.


Create 'VPC Firewall rules' for DSE cluster:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Description:  dse
Network: default
Direction: ingress

Targets: Specified target tags

Target tags: dse-ports

Source filter: IPv4 ranges

# NOTE: beside the first '0.0.0.0/0' the following one to two range values will depend
#       on Public IP addresses of your cluster nodes.
#       (Please enhance this description if you know better)
Source IPv4 ranges: 0.0.0.0/0, 34.124.0.0/24, 34.130.0.0/24

# NOTE: The better choice may be 'Specified protocols and ports' with TCP protocol
#       and
#           9042,9142,7000,7001,7199,10000,9180,9100,9160,19042,19142
#       ports,
#       for the following option.
#       However, I was facing a connectivity issue, and selecting 'Allow all' resolved it for me.
Protocols and Ports: 'Allow all'


Configure firewall rules for DSE cluster nodes:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Edit dse-1, dse-2, and dse-3 nodes (created above) in GCP console,
# and, for each:
Allow HTTP and HTTPS traffic in 'firewalls' section AND add 'dse-ports' tag


Connect to each of the GCP VM instances via SSH from local machine terminal:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# SSH into each of the above 3 VMs to ensure each one has 4 NVMe SSD of 375GB each mounted
# ssh -i ~/.ssh/gcloud-ssh-dse-vms your-gcloud-user-id@vm-public-ip
# For example:
$> ssh -i ~/.ssh/gcloud-ssh-dse-vms syed.junaid@34.130.37.68


List attached NVMe disk drives :
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
$> lsblk

Should have similar to the following as part of the Output
... ... ...
... ... ...
nvme0n1 259:0    0   375G  0 disk
nvme0n2 259:1    0   375G  0 disk
nvme0n3 259:2    0   375G  0 disk
nvme0n4 259:3    0   375G  0 disk
... ... ...
... ... ...


Install Open JDK 8 on each of the 3 dse-x nodes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
REF: https://docs.datastax.com/en/jdk-install/doc/jdk-install/installOpenJdkDeb.html

sudo apt-get update
sudo apt-get install openjdk-8-jdk
java -version

Create RAID0 from attached NVMe disks:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
https://cloud.google.com/compute/docs/disks/add-local-ssd#formatandmount
Please see 'Format and mount multiple local SSD partitions into a single logical volume' section

sudo apt update && sudo apt install mdadm --no-install-recommends

find /dev/ | grep google-local-nvme-ssd

sudo mdadm --create /dev/md0 --level=0 --raid-devices=4 \
 /dev/disk/by-id/google-local-nvme-ssd-0 \
 /dev/disk/by-id/google-local-nvme-ssd-1 \
 /dev/disk/by-id/google-local-nvme-ssd-2 \
 /dev/disk/by-id/google-local-nvme-ssd-3

sudo mdadm --detail --prefer=by-id /dev/md0

sudo mkfs.ext4 -F /dev/md0

sudo mkdir -p /mnt/disks/cassandra_data

sudo mount /dev/md0 /mnt/disks/cassandra_data

sudo chmod a+w /mnt/disks/cassandra_data

echo UUID=`sudo blkid -s UUID -o value /dev/md0` /mnt/disks/cassandra_data ext4 discard,defaults,nofail 0 2 | sudo tee -a /etc/fstab

cat /etc/fstab

Create 'cassandra' User:
~~~~~~~~~~~~~~~~~~~~~~~~~
sudo adduser cassandra

NOTE: provide 'cassandra' as password and leave the following at the default empty (just enter on each)
        Full Name []:
        Room Number []:
        Work Phone []:
        Home Phone []:
        Other []:

sudo usermod -aG sudo cassandra


Create the required folders in RAID0:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
sudo chown -R cassandra:cassandra /mnt/disks/cassandra_data
sudo chmod 770 /mnt/disks/cassandra_data

sudo mkdir /mnt/disks/cassandra_data/data
sudo chown -R cassandra:cassandra /mnt/disks/cassandra_data/data
sudo chmod 755 /mnt/disks/cassandra_data/data

sudo mkdir /mnt/disks/cassandra_data/metadata
sudo chown -R cassandra:cassandra /mnt/disks/cassandra_data/metadata
sudo chmod 755 /mnt/disks/cassandra_data/metadata

sudo mkdir /mnt/disks/cassandra_data/saved_caches
sudo chown -R cassandra:cassandra /mnt/disks/cassandra_data/saved_caches
sudo chmod 755 /mnt/disks/cassandra_data/saved_caches

sudo mkdir /mnt/disks/cassandra_data/cdc_raw
sudo chown -R cassandra:cassandra /mnt/disks/cassandra_data/cdc_raw
sudo chmod 755 /mnt/disks/cassandra_data/cdc_raw

sudo mkdir /mnt/disks/cassandra_data/commitlog
sudo chown -R cassandra:cassandra /mnt/disks/cassandra_data/commitlog
sudo chmod 755 /mnt/disks/cassandra_data/commitlog

sudo mkdir /mnt/disks/cassandra_data/hints
sudo chown -R cassandra:cassandra /mnt/disks/cassandra_data/hints
sudo chmod 755 /mnt/disks/cassandra_data/hints

sudo ls -al /mnt/disks/cassandra_data/



-- --------------------------------------------------------------------------------
-- STEP-3: Installing a DataStax Enterprise cluster using Lifecycle Manager
-- --------------------------------------------------------------------------------

In 'SSH credentials' section (one time):
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
a.) Select 'Private Key' as 'SSH Login' option
b.) Copy 'gcloud-ssh-dse-vms' private key contents in 'SSH Private Key' box
c.) enter 'syed.junaid' in the Login User box
d.) Select 'SUDO' as 'Escalation Privileges' choice and leave the 'SUDO to this user' & 'SUDO password' boxes empty


Configure 'Config Profiles' section (one time):
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Go to 'Config Profiles' section and assign values for the following properties, as shown below:

Name: test
DataStax Enterprise Version: 6.8.33


Go into 'Cassandra.yaml' section and assign values for following properties as shown below:

    authenticator:  AllowAllAuthenticator
    authorizer:     AllowAllAuthorizer
    role_manager:   DseRoleManager

    server_encryption_options:
        internode_encryption:   none

    General:
    partitioner:    Murmer3Partitioner

    data_file_directories:      /mnt/disks/cassandra_data/data
    metadata_directory:         /mnt/disks/cassandra_data/metadata
    saved_caches_directory:     /mnt/disks/cassandra_data/saved_caches
    cdc_raw_directory:          /mnt/disks/cassandra_data/cdc_raw
    commitlog_directory:        /mnt/disks/cassandra_data/commitlog
    hints_directory:            /mnt/disks/cassandra_data/hints

    Cluster Communication:
    endpoint_snitch:    GossipingPropertyFileSnitch



Configure 'Repositories' (one time):
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Go into 'Repositories' in Lifecycle Manager and assign values for following properties as shown below:

Name:  test
Repository:   select 'Acess DataStax repo'
Use Configured HTTP proxy:  leave it CHECKED (i.e. leave it as-is)



Deploy the cluster:
~~~~~~~~~~~~~~~~~~~~
Follow the instructions @
https://docs.datastax.com/en/opscenter/docs/6.8/LCM/opscLCMInstallDSE.html
for this deployment.


In Lifecycle Manager: Clusters definition:

For Clusters:

Name:   dse
SSH Credentials:   Syed Junaid (it will be the user name according to your 'SSH Credentials' settings, see above)

Repository:  test
config Profile: test


For Datacenters:

Name:   improving
Config Profile: test
SSH Credentials:   Syed Junaid (it will be the user name according to your 'SSH Credentials' settings, see above)

Workloads: Cassandra


For Nodes:

Name:   dse-1
        (or dse-2 or dse-3 or something else according the name you assigned to the VM when you created it using gCloud command)

Rack:   a (or b or c)

SSH IP Address:  Private IP address of the VM (see Google Cloud Console for this IP address)

SSH Credentials:   Syed Junaid (it will be the user name according to your 'SSH Credentials' settings, see above)

Config Profile:  test

Seed Node:   Select 'Make this a seed node' option for the first node in the cluster. Select 'Automatically choose' for other nodes.


To initiate the actual installation:

SELECT 'Install' from  '...' menu (next to dse-1 in Nodes list 'Lifecycle Manager:Cluster' deployment UI).
  THEN select 'True' for 'Auto bootstrap' for seed node, and 'Lifecycle Manager Default' for others.



Check directories after deployment:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cat /etc/dse/cassandra/cassandra.yaml |grep director

To find out deployed Linux OS and Version for DSE nodes
$> cat /etc/os-release



-- --------------------------------------------------------------------------------
-- STEP-4: Clearing 'cart_service' data from DataStax Enterprise
-- --------------------------------------------------------------------------------
# Perform the following on every DSE cluster node when no client is connected to the database
nodetool flush
nodetool cleanup

# Peform on one DSE cluster node when no client is connected to the database
cqlsh dse-1 -e "drop keyspace cart_service;"

# Perform the following on every DSE cluster node when no client is connected to the database
nodetool cleanup

sudo ls -al /mnt/disks/cassandra_data/data
sudo rm -r /mnt/disks/cassandra_data/data/cart_service
sudo ls -al /mnt/disks/cassandra_data/data

sudo ls -al /mnt/disks/cassandra_data/commitlog
sudo find /mnt/disks/cassandra_data/commitlog/ -name "CommitLog*" -delete
sudo ls -al /mnt/disks/cassandra_data/commitlog


-- --------------------------------------------------------------------------------
-- STEP-4(Alternative): Clearing ALL the data from DataStax Enterprise
-- NOTE: This will also remove the DSE OpsCenter monitoring dashboard data as well
-- --------------------------------------------------------------------------------


Drain and stop all nodes, starting with the seed node
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
nodetool drain
sudo service dse stop
sudo service dse status

Remove all data
~~~~~~~~~~~~~~~~
sudo su
cd /mnt/disks/cassandra_data/

sudo rm -rf data/* commitlog/* saved_caches/* hints/*
exit

# sudo rm -rf data/* ## Remove only the data directories


Restarting the DataStax Enterprise node
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
sudo service dse start      # sudo service dse restart   ## to restart a dse service
sudo service dse status

# after a minute or so, when all dse processes have started
nodetool status



-- --------------------------------------------------------------------------------
-- STEP-6: Retrieve copy of DSE 'Ops Center' Dashboard data, from 'dse-1' node,
--         So that we can re-review the dashboard at a later time if required
--         by re-installing the DSE OpsCenter and re-building one-node dse Cluster
--         where contents of /mnt/disks/cassandra_data folder in 'dse-1' node are
--         obtained by uncompressing 'mnt_disks_cassandra_data.tar.gz' file.
-- --------------------------------------------------------------------------------

NOTE: Make sure you remove 'cart_service' keyspace and its associated data from
      '/mnt/disk/cassandra_data' folder (as specified in Step-4 above)
      BEFORE
      following this step to retrieve copy of DSE 'Ops Center' Dashboard data

# 'drain' and 'stop' dse-2 and dse-3 nodes from DataStax OpsCenter dashboard, one after another

# 'drain' and 'stop' dse-1 node as the last node (assume 'dse-1' is the seed node)


# ssh into 'dse-1' node
# NOTE: following assume 34.130.248.131 is IP address of 'dse-1' node
~/.ssh/gcloud-ssh-dse-vms syed.junaid@34.130.248.131


# Compress '/mnt/disks/cassandra_data' into a file
sudo tar -zcvf mnt_disks_cassandra_data.tar.gz -C /mnt/disks/cassandra_data .

exit


# Copy 'mnt_disks_cassandra_data.tar.gz' file to your local machine
cd /Users/syed/Work/scylla-vs-cassandra/load-test-results/datastax/DataStax_DSE-1_node_data

gcloud compute scp syed.junaid@dse-1:/home/syed.junaid/mnt_disks_cassandra_data.tar.gz . --ssh-key-file ~/.ssh/gcloud-ssh-dse-vms

NOTE: When done, compare the sized of the 'mnt_disks_cassandra_data.tar.gz' file both on you local machine and 'dse-1' node.
