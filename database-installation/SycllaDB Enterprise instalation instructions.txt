# -------------------------------------------------------------------------------------
# STEP 0: Login & Authentication
# -------------------------------------------------------------------------------------

Authenticate your gcloud SDK from your local machine:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
REF: https://cloud.google.com/artifact-registry/docs/docker/authentication

gcloud auth login


Set the right project:
~~~~~~~~~~~~~~~~~~~~~~~
gcloud projects list |grep scylla
gcloud config set project scylla-cassandra-comparison



-----------------------------------------------------------------------------------------------
STEP-1: Launch the ScyllaDB instances (each instance will be a separate one node cluster)
-----------------------------------------------------------------------------------------------


gcloud compute instances create scylla-1 --image scylladb-enterprise-2022-2-3 --image-project scylla-images --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --machine-type=n2-highmem-4 --zone=northamerica-northeast2-a

gcloud compute instances create scylla-2 --image scylladb-enterprise-2022-2-3 --image-project scylla-images --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --machine-type=n2-highmem-4 --zone=northamerica-northeast2-a

gcloud compute instances create scylla-3 --image scylladb-enterprise-2022-2-3 --image-project scylla-images --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --local-ssd interface=nvme --machine-type=n2-highmem-4 --zone=northamerica-northeast2-a



NOTE:
There is no option to specify the local-ssd size. It is fixed to 375GB. If you need more local ssd then repeat '--local-ssd interface=nvme' to achieve a desired size of total attached local ssd capacity.


Configure the VMs for SSH connection:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# If not already done, Generate SSH keys on your local machine:
ssh-keygen -t rsa -f ~/.ssh/gcloud-ssh-dse-vms -C syed.junaid@yoppworks.com
ssh-keygen -p -N "" -m pem -f ~/.ssh/gcloud-ssh-dse-vms

# Transfer Public Key to the VM instance(s) once they are created:
# Print public key
$> cat ~/.ssh/gcloud-ssh-dse-vms.pub

# <Edit> your VM instance in Google Cloud console and select <ADD ITEM> under 'SSH Keys', and copy your Public Key (from previous step) there.



Configure firewall rules for ScyllaDB cluster nodes:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
REF: https://docs.scylladb.com/stable/operating-scylla/admin   (see 'Networking' section)
HOW TO HELP: https://www.howtogeek.com/devops/how-to-open-firewall-ports-on-a-gcp-compute-engine-instance/

# Edit scylla-1, scylla-2, and scylla-3 nodes (created above) in GCP console,
# and, for each:
Allow HTTP and HTTPS traffic in 'firewalls' section AND add 'scylla-ports' tag



Connect to each of the GCP VM instances via SSH from local machine terminal:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# SSH into each of the above 3 VMs to ensure each one has 4 NVMe SSD of 375GB each mounted
# ssh -i ~/.ssh/gcloud-ssh-dse-vms your-gcloud-user-id@vm-public-ip
# For example:
$> ssh -i ~/.ssh/gcloud-ssh-dse-vms syed.junaid@34.130.37.68


List attached NVMe disk drives:
$> lsblk


To find out deployed Linux OS and Version for DSE nodes
$> cat /etc/os-release


-----------------------------------------------------------------------------------------------------
STEP-2: Stop each individual ScyllaDB instance and clear the data
-----------------------------------------------------------------------------------------------------
REF: https://docs.scylladb.com/stable/operating-scylla/procedures/cluster-management/clear-data.html


sudo systemctl stop scylla-server
sudo systemctl status scylla-server

ls -al /var/lib/scylla/data

sudo rm -rf /var/lib/scylla/data
sudo find /var/lib/scylla/commitlog -type f -delete
sudo find /var/lib/scylla/hints -type f -delete
sudo find /var/lib/scylla/view_hints -type f -delete


-----------------------------------------------------------------------------------------------------
STEP-3: Perform the cluster configuration on each of the nodes
-----------------------------------------------------------------------------------------------------
REF: https://docs.scylladb.com/stable/operating-scylla/procedures/cluster-management/create-cluster.html

sudo nano /etc/scylla/scylla.yaml

make:
     cluster_name: scylla
     endpoint_snitch: GossipingPropertyFileSnitch
     auto_bootstrap: true       # for the first/seed node, false for others
     seed_provider:
       - seeds: 10.188.0.62     # add private IP address of the first/seed node

leave as-is:
            rpc_address: 0.0.0.0


Among other settings as directed in the REF: link above.

NOTE: Recommendations from ScyllaDB support team for deploying all ScyllaDB nodes in a single zone
Manually set endpoint_snitch: GossipingPropertyFileSnitch and force artificial racks using cassandra-rackdc.properties file for each node. In the end, you should still have number of racks matching the replication factor



sudo nano /etc/scylla/cassandra-rackdc.properties

make:
     dc=yoppworks
     rack=x  ,  rack=y ,  rack=z
                                for 1st, 2nd, and 3rd node respectively


-----------------------------------------------------------------------------------------------------
STEP-3.1: OPTIONALLY???  run scylla_setup
-----------------------------------------------------------------------------------------------------
cd /opt/scylladb/scripts/
sudo ./scylla_setup


--------------------------------------------------------------------------------------
STEP-4: Bring all the nodes back-up, one after another, starting with the seed node
--------------------------------------------------------------------------------------
sudo systemctl start scylla-server
sudo systemctl status scylla-server



--------------------------------------------------------------------------------------
STEP-5: Deploy ScyllaDB Monitoring Stack
--------------------------------------------------------------------------------------
REF: https://monitoring.docs.scylladb.com/stable/install/monitoring_stack.html

To find out deployed Linux OS and Version for ScyllaDB nodes
$> cat /etc/os-release

NOTE: Output from ScyllaDB nodes (deployed above)
NAME="Ubuntu"
VERSION="20.04.5 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.5 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal



Scylla Monitoring - Minimal Production System Recommendations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
REF: https://monitoring.docs.scylladb.com/stable/install/monitoring_stack.html#minimal-production-system-recommendations

Minimum CPU: 4 vCPUs
Minimum Memory: 15GB+ DRAM and proportional to the number of cores.
Network: 10GbE preferred


To work with the 3-node scyllaDB cluster or 'n2-highmem-4' nodes:
For Prometheus - Min RAM: 720 MB
               - Min Storage 2.11 GB (for 15-days retention)

Based on the ScyllaDB recommendations for monitoring stack, we will deploy Scylla Monitoring stack on 'n2-standard-4' instance with 2.5 GB for standard HD.


Check available standard OS images in GCP:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
$> gcloud compute images list --standard-images |grep ubuntu
... ... ...
... ... ...
NAME: ubuntu-2004-focal-v20230302
PROJECT: ubuntu-os-cloud
FAMILY: ubuntu-2004-lts
... ... ...
... ... ...


Check available storage disks in 'northamerica-northeast2-a' zone in GCP:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
$> gcloud compute disk-types list --zones=northamerica-northeast2-a
... ... ...
... ... ...
NAME: pd-standard
ZONE: northamerica-northeast2-a
VALID_DISK_SIZES: 10GB-65536GB
... ... ...
... ... ...



Instantiate 'n2-standard-4' VM in 'northamerica-northeast2-a' zone in GCP:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gcloud compute instances create scylla-monitoring-2 --image ubuntu-2004-focal-v20230302 --image-project ubuntu-os-cloud --boot-disk-size=2500GB --boot-disk-type=pd-standard --machine-type=n2-standard-4 --zone=northamerica-northeast2-a



Transfer Public Key to the Scylla Monitoring instance(s) once they are created:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Print public key
cat ~/.ssh/gcloud-ssh-dse-vms.pub


# <Edit> your VM instance in Google Cloud console and select <ADD ITEM> under 'SSH Keys', and copy your Public Key (from previous step) there.


Install Docker:
~~~~~~~~~~~~~~~~
Open the SSH command shell on 'scylla-monitoring' VM instance, and use the following commands to install docker.
Ref: https://docs.docker.com/engine/install/ubuntu/

Perform the following 'Docker post install' steps to avoid running docker as root, you should add the user you are going to use for Scylla Monitoring Stack to the Docker group.

sudo groupadd docker
sudo usermod -aG docker $USER
sudo systemctl enable docker



Finally to install Scylla Monitoring Stack:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Download and extract the latest Scylla Monitoring Stack binary
wget https://github.com/scylladb/scylla-monitoring/archive/scylla-monitoring-4.3.0.tar.gz
tar -xvf scylla-monitoring-4.3.0.tar.gz
cd scylla-monitoring-scylla-monitoring-4.3.0


# Start Docker service if needed
sudo systemctl restart docker


# Create the scylla_servers.yml file:
cp prometheus/scylla_servers.example.yml prometheus/scylla_servers.yml

              with contents similar to the following:
              # List Scylla end points

              - targets:
                     - 34.130.111.230:9180
                labels:
                     cluster: scylla
                     dc: yoppworks


# OPTIONAL: To stop/remove the existing monitoring stack
./kill-all.sh
cd ~/scylla-monitoring-scylla-monitoring-4.3.0
rm -r prometheus_data/

# Start the Scylla Monitoring Stack:
cd ~/scylla-monitoring-scylla-monitoring-4.3.0
./start-all.sh -s prometheus/scylla_servers.yml -d prometheus_data





# Open port 3000 and 1514 on Scylla Monitoring Node(VM)
# by creating a Firewall rule name 'scylla-monitor' and Targets 'scylla-monitor-ports' that allow Ingress TCP traffic on PORTs 1514 & 3000.



Configure rsyslog on each Scylla node to generates metrics and alerts from logs
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
First, install 'rsyslog':
(REF: https://www.rsyslog.com/doc/v8-stable/installation/index.html)
$> sudo apt-get install rsyslog


Add scyllaâ€™s rsyslog configuration file 'scylla.conf' at location '/etc/rsyslog.d/' on each of the ScyllaDB VMs.
This file should have the following contents:

         if $programname ==  'scylla' then @@34.124.126.227:1514;RSYSLOG_SyslogProtocol23Format

NOTE: 34.124.126.227 is the public IP address of the Scylla Monitoring Stack node


Once /etc/rsyslog.d/scylla.conf is available, restart the rsyslog using following command:
sudo systemctl restart rsyslog




Launch the Scylla Monitor:
~~~~~~~~~~~~~~~~~~~~~~~~~~~
Visit web page @ http://34.124.126.227:3000

NOTE: 34.124.126.227 is the public IP address of the Scylla Monitoring Stack node
